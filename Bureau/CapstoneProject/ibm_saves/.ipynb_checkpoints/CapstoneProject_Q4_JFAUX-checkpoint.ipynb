{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CapstoneProject | Answering question # 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can a predictive model be built for a future prediction of the possibility of complaints of the type that you have identified in response to question 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import types\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2 aka NYC complaints OK\n",
      "df_data_3 aka pluto housing dataset OK\n"
     ]
    }
   ],
   "source": [
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_2a47d0a6f5c648e09ef1086c5031e6ea = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='',\n",
    "    ibm_auth_endpoint=\"\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_2a47d0a6f5c648e09ef1086c5031e6ea.get_object(Bucket='',Key='ny_complaints.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body)\n",
    "\n",
    "df_data_2 = pd.read_csv(body)\n",
    "print('df_data_2 aka NYC complaints OK')\n",
    "\n",
    "# @hidden_cell\n",
    "body2 = client_2a47d0a6f5c648e09ef1086c5031e6ea.get_object(Bucket='',Key='pluto_19v2.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body2)\n",
    "df_data_3 = pd.read_csv(body2, low_memory=False)\n",
    "print('df_data_3 aka pluto housing dataset OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting data, same lines as in notebooks 2 and 3, plus we'll had borough's dummy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data_2 dropna...\n",
      "df_data_2 is_heat_the_problem creation...\n",
      "df_data_2 address formatting...\n",
      "df_data_3 address formatting...\n",
      "df_data_3 dropna...\n",
      "merging...\n",
      "df_data_2 is ready ;)\n"
     ]
    }
   ],
   "source": [
    "df_data_2 = df_data_2[[\"complaint_type\",\"incident_zip\",\"incident_address\",\"street_name\", \"latitude\", \"longitude\"]]\n",
    "print('df_data_2 dropna...')\n",
    "df_data_2 = df_data_2.dropna()\n",
    "print('df_data_2 is_heat_the_problem creation...')\n",
    "df_data_2['is_heat_the_problem'] = np.where((df_data_2['complaint_type']=='HEAT/HOT WATER') | (df_data_2['complaint_type']=='HEATING'), True, False)\n",
    "print('df_data_2 address formatting...')\n",
    "df_data_2['incident_address'] = [' '.join(str(mystring).split()) for mystring in df_data_2['incident_address']]\n",
    "print('df_data_3 address formatting...')\n",
    "df_data_3.address = [' '.join(str(mystring).split()) for mystring in df_data_3.address]\n",
    "df_data_3 = df_data_3[['borough', 'schooldist', 'address', 'numfloors', 'yearbuilt']]\n",
    "print('df_data_3 dropna...')\n",
    "df_data_3 = df_data_3.dropna()\n",
    "print(\"merging...\")\n",
    "df_data_2 = pd.merge(df_data_2, df_data_3, how='left', left_on=['incident_address'], right_on = ['address'])\n",
    "df_data_2 = df_data_2.dropna()\n",
    "df_data_2 = pd.concat([df_data_2, pd.get_dummies(df_data_2.borough)], axis=1)\n",
    "df_data_2.groupby(['borough', 'street_name'])\n",
    "print(\"df_data_2 is ready ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "COL_LIST=['incident_zip', 'numfloors', 'schooldist', 'yearbuilt']\n",
    "REQUIRED = ['complaint_type', 'address', 'street_name', 'latitude', 'longitude']\n",
    "df_Y = ['is_heat_the_problem']\n",
    "\n",
    "def return_specific_dataframe(borough=None, random_sampling=-1, sampling_size=-1):\n",
    "    df = df_data_2\n",
    "    if borough is not None:\n",
    "        df = df_data_2[df_data_2.borough ==borough]\n",
    "    if random_sampling >0:\n",
    "        df = df.sample(frac=random_sampling)\n",
    "    df.groupby('incident_address')\n",
    "    if sampling_size >0:\n",
    "        df = df[:sampling_size]\n",
    "\n",
    "    return df[df_Y+COL_LIST+REQUIRED]\n",
    "    \n",
    "targeted_boroughs = ['BK', 'BX', 'MN', 'QN', 'SI']\n",
    "def random_forest_predictor(df):\n",
    "    #print('working with %s at %s%% sampling rate and %s test size'% (targeted_borough, 100*sampling_rate, test_size))\n",
    "    print(\"---RUNNING ON %s DOCUMENTS---\" % len(df))\n",
    "    \n",
    "    \n",
    "    X = np.asarray(df[COL_LIST])\n",
    "    X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "    Y = np.asarray(df[['is_heat_the_problem']])\n",
    "\n",
    "    corpus = np.asarray(df['address'])\n",
    "    #print(corpus[0])\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=1)\n",
    "\n",
    "\n",
    "    clf = RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                 criterion='gini', max_depth=None, max_features='auto',\n",
    "                                 max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, \n",
    "                                 min_impurity_split=None,\n",
    "                                 min_samples_leaf=1, min_samples_split=2,\n",
    "                                 min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                 n_jobs=8, oob_score=False, random_state=None,\n",
    "                                 verbose=0, warm_start=False)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "    y_hat = clf.predict(X_test)\n",
    "    print(\"Train set Accuracy: %.3f\" % metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "    print(\"Test set Accuracy: %.3f\" % metrics.accuracy_score(y_test, y_hat))\n",
    "    print(\"Precision: %.3f\" % metrics.precision_score(y_test, y_hat))\n",
    "    print(\"Recall: %.3f\" % metrics.recall_score(y_test, y_hat))\n",
    "    print(\"confusion matrix: \\n%s\" % metrics.confusion_matrix(y_test, y_hat, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RANDOM SAMPLING IN DATAFRAME\n",
      "1.a with specific borough\n",
      "\n",
      "BOROUGH BK\n",
      "---RUNNING ON 5494 DOCUMENTS---\n",
      "Train set Accuracy: 0.923\n",
      "Test set Accuracy: 0.673\n",
      "Precision: 0.480\n",
      "Recall: 0.236\n",
      "confusion matrix: \n",
      "[[ 83 269]\n",
      " [ 90 657]]\n",
      "\n",
      "\n",
      "BOROUGH BX\n",
      "---RUNNING ON 4887 DOCUMENTS---\n",
      "Train set Accuracy: 0.886\n",
      "Test set Accuracy: 0.661\n",
      "Precision: 0.487\n",
      "Recall: 0.291\n",
      "confusion matrix: \n",
      "[[ 95 232]\n",
      " [100 551]]\n",
      "\n",
      "\n",
      "BOROUGH MN\n",
      "---RUNNING ON 3170 DOCUMENTS---\n",
      "Train set Accuracy: 0.909\n",
      "Test set Accuracy: 0.596\n",
      "Precision: 0.463\n",
      "Recall: 0.303\n",
      "confusion matrix: \n",
      "[[ 74 170]\n",
      " [ 86 304]]\n",
      "\n",
      "\n",
      "BOROUGH QN\n",
      "---RUNNING ON 1824 DOCUMENTS---\n",
      "Train set Accuracy: 0.950\n",
      "Test set Accuracy: 0.652\n",
      "Precision: 0.492\n",
      "Recall: 0.254\n",
      "confusion matrix: \n",
      "[[ 32  94]\n",
      " [ 33 206]]\n",
      "\n",
      "\n",
      "BOROUGH SI\n",
      "---RUNNING ON 338 DOCUMENTS---\n",
      "Train set Accuracy: 0.959\n",
      "Test set Accuracy: 0.721\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "confusion matrix: \n",
      "[[ 0 14]\n",
      " [ 5 49]]\n",
      "\n",
      "1.b without specifying a specific borough\n",
      "---RUNNING ON 15714 DOCUMENTS---\n",
      "Train set Accuracy: 0.914\n",
      "Test set Accuracy: 0.620\n",
      "Precision: 0.450\n",
      "Recall: 0.291\n",
      "confusion matrix: \n",
      "[[ 326  796]\n",
      " [ 399 1622]]\n",
      "\n",
      "2. SAMPLING IN DATAFRAME BY SORTED ADDRESS\n",
      "2.a with specific borough\n",
      "\n",
      "BOROUGH BK\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.854\n",
      "Test set Accuracy: 0.728\n",
      "Precision: 0.738\n",
      "Recall: 0.751\n",
      "confusion matrix: \n",
      "[[2380  787]\n",
      " [ 845 1988]]\n",
      "\n",
      "\n",
      "BOROUGH BX\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.831\n",
      "Test set Accuracy: 0.733\n",
      "Precision: 0.749\n",
      "Recall: 0.761\n",
      "confusion matrix: \n",
      "[[2466  776]\n",
      " [ 826 1932]]\n",
      "\n",
      "\n",
      "BOROUGH MN\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.828\n",
      "Test set Accuracy: 0.743\n",
      "Precision: 0.775\n",
      "Recall: 0.841\n",
      "confusion matrix: \n",
      "[[3220  607]\n",
      " [ 936 1237]]\n",
      "\n",
      "\n",
      "BOROUGH QN\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.847\n",
      "Test set Accuracy: 0.740\n",
      "Precision: 0.759\n",
      "Recall: 0.784\n",
      "confusion matrix: \n",
      "[[2636  728]\n",
      " [ 835 1801]]\n",
      "\n",
      "\n",
      "BOROUGH SI\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.838\n",
      "Test set Accuracy: 0.808\n",
      "Precision: 0.660\n",
      "Recall: 0.410\n",
      "confusion matrix: \n",
      "[[ 589  847]\n",
      " [ 303 4261]]\n",
      "\n",
      "2.b without specifying a specific borough\n",
      "---RUNNING ON 30000 DOCUMENTS---\n",
      "Train set Accuracy: 0.922\n",
      "Test set Accuracy: 0.842\n",
      "Precision: 0.449\n",
      "Recall: 0.268\n",
      "confusion matrix: \n",
      "[[ 240  654]\n",
      " [ 294 4812]]\n"
     ]
    }
   ],
   "source": [
    "SAMPLER=0.003\n",
    "SAMPLING_SIZE=30000\n",
    "test_sizes=[.2]\n",
    "\n",
    "print(\"1. RANDOM SAMPLING IN DATAFRAME\")\n",
    "print(\"1.a with specific borough\")\n",
    "for borough in targeted_boroughs:\n",
    "    print('\\nBOROUGH %s' % borough)\n",
    "    random_forest_predictor(return_specific_dataframe(borough=borough, random_sampling=SAMPLER))\n",
    "    print('')\n",
    "print(\"1.b without specifying a specific borough\")\n",
    "random_forest_predictor(return_specific_dataframe(borough=None, random_sampling=SAMPLER))\n",
    "print('')\n",
    "print(\"2. SAMPLING IN DATAFRAME BY SORTED ADDRESS\")\n",
    "print(\"2.a with specific borough\")\n",
    "for borough in targeted_boroughs:\n",
    "    print('\\nBOROUGH %s' % borough)\n",
    "    random_forest_predictor(return_specific_dataframe(borough=borough, random_sampling=-1, sampling_size=SAMPLING_SIZE))\n",
    "    print('')\n",
    "print(\"2.b without specifying a specific borough\")\n",
    "random_forest_predictor(return_specific_dataframe(borough=None, random_sampling=-1, sampling_size=SAMPLING_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix, precision and recall results **of the 2.a approach are more than acceptable** as a proof that **prediction is possible in a sorted address dataframe.**\n",
    "The main tricks that allowed this achievement are \n",
    "- proper sampling size \n",
    "- full address vectorization as the main feature to be used in random forest classifier\n",
    "\n",
    "grouping by incident address is a poor hack showing that prediction is possible having minimum amount of same addresses and minimum documents number. \n",
    "\n",
    "Thank you for reading this, I am sure you did great as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
